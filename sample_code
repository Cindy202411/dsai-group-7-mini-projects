import requests
import csv
from bs4 import BeautifulSoup
import time

# Base Amazon Books URL (Page 1)
base_url = "https://www.amazon.sg/s?k=books&page="

# Fake User-Agent to avoid detection
headers = {"User-Agent": "Mozilla/5.0"}

# File to store data
csv_filename = "amazon_books_all_pages.csv"

# Open CSV file and write headers
with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["Title", "Price", "Rating"])  # CSV Header

    # Loop through multiple pages (Change range as needed)
    for page in range(1, 6):  # Scrapes first 5 pages, adjust as needed
        print(f"Scraping Page {page}...")

        # Construct the URL for each page
        url = f"{base_url}{page}"
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")

        # Extract books data
        books = []
        for book, price, rating in zip(
            soup.select('.s-title-instructions-style'),
            soup.select('.a-price-whole'),
            soup.select('.a-icon-alt')
        ):
            title = book.get_text(strip=True)
            price_text = price.get_text(strip=True)
            rating_text = rating.get_text(strip=True)

            books.append([title, price_text, rating_text])

        # Append to CSV file
        writer.writerows(books)

        # Pause to avoid detection (Adjust delay if needed)
        time.sleep(2)

print(f"Data saved to {csv_filename}")
